{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d47998b",
   "metadata": {},
   "source": [
    "# Label scores analyses\n",
    "\n",
    "This notebook performs the following analysis steps:\n",
    "* aggregating data across seeds, by phenomenon, by model, by label type\n",
    "* transforming raw metrics into probabilities by item by metric\n",
    "    * this requires working by-phenomenon, because there are different numbers of options in the different phenomena\n",
    "* combining this data into one cleaned data file with binarized options (probabilities of target vs rest)\n",
    "    * long formatting of options and of metrics\n",
    "* averaging probabilities across items and across seeds, by metric, by model, by phenomenon, by label type\n",
    "* plotting results by metric, by phenomenon, by model, by label type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ff26aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9706a59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  llama2\n",
      "Number of files to concat  35\n",
      "Number of files to concat  35\n",
      "Model  flan-t5-xl\n",
      "Number of files to concat  35\n",
      "Number of files to concat  35\n",
      "Model  gpt-3.5-turbo-instruct\n",
      "Number of files to concat  35\n",
      "Number of files to concat  31\n"
     ]
    }
   ],
   "source": [
    "# read data and combine\n",
    "model_names = [\"llama2\", \"flan-t5-xl\", \"gpt-3.5-turbo-instruct\"]\n",
    "label_types = [\"label_alpha\", \"label_numeric\"]\n",
    "\n",
    "raw_df = pd.DataFrame()\n",
    "for model in model_names:\n",
    "    print(\"Model \", model)\n",
    "    for label_type in label_types:\n",
    "        results_path = glob.glob(f\"../results/log_probs/{model}/*/label_scores/{label_type}/*.csv\")\n",
    "        print(\"Number of files to concat \", len(results_path))\n",
    "        for p in results_path:\n",
    "#             print(\"Reading file \", p)\n",
    "            d = pd.read_csv(p)\n",
    "            d[\"metric\"] = \"label_score\"\n",
    "            d[\"label_type\"] = label_type\n",
    "            \n",
    "            raw_df = pd.concat([raw_df, d])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d0d49607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19330\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>temperature</th>\n",
       "      <th>seed</th>\n",
       "      <th>item_id</th>\n",
       "      <th>phenomenon</th>\n",
       "      <th>prompt</th>\n",
       "      <th>prior_prompt</th>\n",
       "      <th>question</th>\n",
       "      <th>options</th>\n",
       "      <th>option_names</th>\n",
       "      <th>...</th>\n",
       "      <th>token_probs</th>\n",
       "      <th>sentence_cond_probs</th>\n",
       "      <th>mean_sentence_cond_probs</th>\n",
       "      <th>prior_sentence_probs</th>\n",
       "      <th>sentence_mi</th>\n",
       "      <th>sentence_surprisal</th>\n",
       "      <th>mean_sentence_surprisal</th>\n",
       "      <th>sentence_mi_surprisal</th>\n",
       "      <th>metric</th>\n",
       "      <th>label_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meta-llama/Llama-2-7b-hf</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>humour</td>\n",
       "      <td>You will read a joke that is missing its punch...</td>\n",
       "      <td>You will read a joke that is missing its punch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>target</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0021805406873923253]</td>\n",
       "      <td>0.003313</td>\n",
       "      <td>0.003313</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>1.519248</td>\n",
       "      <td>-5.709967</td>\n",
       "      <td>-5.709967</td>\n",
       "      <td>0.931755</td>\n",
       "      <td>label_score</td>\n",
       "      <td>label_alpha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>meta-llama/Llama-2-7b-hf</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>humour</td>\n",
       "      <td>You will read a joke that is missing its punch...</td>\n",
       "      <td>You will read a joke that is missing its punch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>incorrect_straightforward</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0018741641342209612]</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>1.103044</td>\n",
       "      <td>-6.181519</td>\n",
       "      <td>-6.181519</td>\n",
       "      <td>0.984382</td>\n",
       "      <td>label_score</td>\n",
       "      <td>label_alpha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>meta-llama/Llama-2-7b-hf</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>humour</td>\n",
       "      <td>You will read a joke that is missing its punch...</td>\n",
       "      <td>You will read a joke that is missing its punch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>associative_nonsequitur</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.003587298613933627]</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.003587</td>\n",
       "      <td>0.458951</td>\n",
       "      <td>-6.409168</td>\n",
       "      <td>-6.409168</td>\n",
       "      <td>1.138324</td>\n",
       "      <td>label_score</td>\n",
       "      <td>label_alpha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>meta-llama/Llama-2-7b-hf</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>humour</td>\n",
       "      <td>You will read a joke that is missing its punch...</td>\n",
       "      <td>You will read a joke that is missing its punch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>funny_nonsequitur</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.002218916267492813]</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.812816</td>\n",
       "      <td>-6.317986</td>\n",
       "      <td>-6.317986</td>\n",
       "      <td>1.033916</td>\n",
       "      <td>label_score</td>\n",
       "      <td>label_alpha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meta-llama/Llama-2-7b-hf</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>humour</td>\n",
       "      <td>You will read a joke that is missing its punch...</td>\n",
       "      <td>You will read a joke that is missing its punch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E</td>\n",
       "      <td>neutral_nonsequitur</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0014657024465339016]</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.671308</td>\n",
       "      <td>-6.923948</td>\n",
       "      <td>-6.923948</td>\n",
       "      <td>1.061073</td>\n",
       "      <td>label_score</td>\n",
       "      <td>label_alpha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model_name  temperature  seed  item_id phenomenon  \\\n",
       "0  meta-llama/Llama-2-7b-hf          0.1     3        1     humour   \n",
       "1  meta-llama/Llama-2-7b-hf          0.1     3        1     humour   \n",
       "2  meta-llama/Llama-2-7b-hf          0.1     3        1     humour   \n",
       "3  meta-llama/Llama-2-7b-hf          0.1     3        1     humour   \n",
       "4  meta-llama/Llama-2-7b-hf          0.1     3        1     humour   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  You will read a joke that is missing its punch...   \n",
       "1  You will read a joke that is missing its punch...   \n",
       "2  You will read a joke that is missing its punch...   \n",
       "3  You will read a joke that is missing its punch...   \n",
       "4  You will read a joke that is missing its punch...   \n",
       "\n",
       "                                        prior_prompt question options  \\\n",
       "0  You will read a joke that is missing its punch...      NaN       A   \n",
       "1  You will read a joke that is missing its punch...      NaN       B   \n",
       "2  You will read a joke that is missing its punch...      NaN       C   \n",
       "3  You will read a joke that is missing its punch...      NaN       D   \n",
       "4  You will read a joke that is missing its punch...      NaN       E   \n",
       "\n",
       "                option_names  ...              token_probs  \\\n",
       "0                     target  ...  [0.0021805406873923253]   \n",
       "1  incorrect_straightforward  ...  [0.0018741641342209612]   \n",
       "2    associative_nonsequitur  ...   [0.003587298613933627]   \n",
       "3          funny_nonsequitur  ...   [0.002218916267492813]   \n",
       "4        neutral_nonsequitur  ...  [0.0014657024465339016]   \n",
       "\n",
       "  sentence_cond_probs mean_sentence_cond_probs prior_sentence_probs  \\\n",
       "0            0.003313                 0.003313             0.002181   \n",
       "1            0.002067                 0.002067             0.001874   \n",
       "2            0.001646                 0.001646             0.003587   \n",
       "3            0.001804                 0.001804             0.002219   \n",
       "4            0.000984                 0.000984             0.001466   \n",
       "\n",
       "  sentence_mi sentence_surprisal mean_sentence_surprisal  \\\n",
       "0    1.519248          -5.709967               -5.709967   \n",
       "1    1.103044          -6.181519               -6.181519   \n",
       "2    0.458951          -6.409168               -6.409168   \n",
       "3    0.812816          -6.317986               -6.317986   \n",
       "4    0.671308          -6.923948               -6.923948   \n",
       "\n",
       "  sentence_mi_surprisal       metric   label_type  \n",
       "0              0.931755  label_score  label_alpha  \n",
       "1              0.984382  label_score  label_alpha  \n",
       "2              1.138324  label_score  label_alpha  \n",
       "3              1.033916  label_score  label_alpha  \n",
       "4              1.061073  label_score  label_alpha  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(raw_df))\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6451f432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['model_name', 'temperature', 'seed', 'item_id', 'phenomenon', 'prompt',\n",
       "       'prior_prompt', 'question', 'options', 'option_names',\n",
       "       'shuffled_options', 'shuffled_option_names', 'option_numbering',\n",
       "       'token_cond_log_probs', 'token_cond_probs', 'prior_token_log_probs',\n",
       "       'null_prior_token_log_probs', 'token_probs', 'sentence_cond_probs',\n",
       "       'mean_sentence_cond_probs', 'prior_sentence_probs', 'sentence_mi',\n",
       "       'sentence_surprisal', 'mean_sentence_surprisal',\n",
       "       'sentence_mi_surprisal', 'metric', 'label_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "752db57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_df = raw_df[raw_df['phenomenon'] == 'coherence']\n",
    "deceits_df = raw_df[raw_df['phenomenon'] == 'deceits']\n",
    "humours_df = raw_df[raw_df['phenomenon'] == 'humour']\n",
    "indirect_df = raw_df[raw_df['phenomenon'] == 'indirect_speech']\n",
    "irony_df = raw_df[raw_df['phenomenon'] == 'irony']\n",
    "maxims_df = raw_df[raw_df['phenomenon'] == 'maxims']\n",
    "metaphor_df = raw_df[raw_df['phenomenon'] == 'metaphor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53df6a2",
   "metadata": {},
   "source": [
    "For the sanity check, here are the expected numbers of rows in each phenomenon, computed as: models * seed * items * options\n",
    "\n",
    "* coherence: 3 * 5 * 40 * 2 = 1200 => 2400\n",
    "* deceits: 3 * 5 * 20 * 4 = 1200 => 2400\n",
    "* humour: 3 * 5 * 25 * 5 = 1875 => 3750\n",
    "* indirect: 3 * 5 * 20 * 4 = 1200 => 2400\n",
    "* irony: 3 * 5 * 25 * 4 = 1500 => 3000\n",
    "* maxims: 3 * 5 * 19 * 4 = 1140 => 2280\n",
    "* metaphor: 3 * 5 * 20 * 5 = 1500 => 3000\n",
    "\n",
    "And for each x 2 for the numeric and alpha labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "27659927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# somehow irony has 100 duplicate entries -- exclude\n",
    "irony_df = irony_df.drop_duplicates()\n",
    "len(irony_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1006f7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_wide_and_softmax(df, option_names = ['target', 'competitor']):\n",
    "    \"\"\"\n",
    "    Helper function for pivoting dataframes \n",
    "    to wide format and transforming metric results into \n",
    "    probabilities.\n",
    "    Metric columns are collected into long format first for this.\n",
    "    \"\"\"\n",
    "    # pivot all the metrics into one column\n",
    "    df_subset = df[[\"model_name\", \"seed\", \"item_id\", \"phenomenon\", \"shuffled_option_names\", \"sentence_cond_probs\", \"mean_sentence_cond_probs\", \"sentence_mi\", \"sentence_surprisal\", \"mean_sentence_surprisal\", \"sentence_mi_surprisal\", \"metric\", \"label_type\"]]\n",
    "    df_subset = df_subset.rename(columns={\n",
    "        \"sentence_cond_probs\": \"value_sentence_cond_probs\", \n",
    "        \"mean_sentence_cond_probs\": \"value_mean_sentence_cond_probs\", \n",
    "        \"sentence_mi\": \"value_sentence_mi\", \n",
    "        \"sentence_surprisal\": \"value_sentence_surprisal\", \n",
    "        \"mean_sentence_surprisal\": \"value_mean_sentence_surprisal\", \n",
    "        \"sentence_mi_surprisal\": \"value_sentence_mi_surprisal\"\n",
    "    })\n",
    "#     print(df_subset.head(5))\n",
    "    df_long = pd.wide_to_long(\n",
    "        df_subset, \n",
    "        stubnames=\"value_\", \n",
    "        i=[\"model_name\", \"seed\", \"item_id\", \"phenomenon\", \"shuffled_option_names\", \"metric\", \"label_type\"], \n",
    "        j=\"metric_formula\",\n",
    "        suffix='\\\\w+', \n",
    "    )\n",
    "    df_long = df_long.reset_index()\n",
    "    # fill NAs with 0\n",
    "    df_long['value_'] = df_long[\"value_\"].fillna(0)\n",
    "    \n",
    "    # spread different response options so that softmax can be applied\n",
    "    df_wide = df_long.pivot_table(\n",
    "        index=[\"model_name\", \"seed\", \"item_id\", \"phenomenon\",\"metric\", \"label_type\", \"metric_formula\"],\n",
    "        columns=\"shuffled_option_names\",\n",
    "        values=\"value_\"\n",
    "    )\n",
    "    df_wide = df_wide.reset_index()\n",
    "    \n",
    "    # softmax options over response option types\n",
    "    df_wide[\"denominator\"] = df_wide[option_names].sum(axis=1)\n",
    "    for o in option_names:\n",
    "        df_wide[o] = df_wide[o]/df_wide[\"denominator\"]\n",
    "        \n",
    "    return df_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c3fc2645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "meta-llama/Llama-2-7b-hf    800\n",
       "gpt-3.5-turbo-instruct      640\n",
       "google/flan-t5-xl           320\n",
       "Name: model_name, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(irony_df)\n",
    "#2300 / 3 / 25\n",
    "# models, items, labels, seeds\n",
    "3 * 25 * 2 * 5 * 4\n",
    "indirect_df[\"model_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "816fd56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the helper to all phenomena\n",
    "coherence_processed = transform_to_wide_and_softmax(coherence_df)\n",
    "deceits_processed = transform_to_wide_and_softmax(deceits_df, [\"target\", \"incorrect_literal\", \"incorrect_lexical_overlap\", \"incorrect_social_convention\"])\n",
    "humours_processed = transform_to_wide_and_softmax(humours_df, [\"target\", \"incorrect_straightforward\", \"associative_nonsequitur\", \"funny_nonsequitur\", \"neutral_nonsequitur\"])\n",
    "indirect_processed = transform_to_wide_and_softmax(indirect_df, [\"target\", \"competitor\", \"distractor_lexical_overlap\", \"distractor_associative\"])\n",
    "irony_processed = transform_to_wide_and_softmax(irony_df, [\"target\", \"competitor\", \"distractor_associate\", \"distractor_nonsequitur\"])\n",
    "maxims_processed = transform_to_wide_and_softmax(maxims_df, [\"target\", \"incorrect_literal\", \"incorrect_nonliteral\", \"incorrect_associate\"])\n",
    "metaphor_processed = transform_to_wide_and_softmax(metaphor_df, [\"target\", \"competitor\", \"distractor_plausibleliteral\", \"distractor_literal\", \"distractor_nonsequitut\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c2f48cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize the processed data into target vs non-target probability\n",
    "def binarize_df(df):\n",
    "    df[\"distractor_prob\"] = 1 - df[\"target\"]\n",
    "    df_subset = df[['metric_formula', \"model_name\", \"seed\", \"item_id\", \"phenomenon\", \"metric\", \"label_type\", \"target\", \"distractor_prob\"]]\n",
    "    \n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a4fe2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_binary = binarize_df(coherence_processed)\n",
    "deceits_binary = binarize_df(deceits_processed)\n",
    "humours_binary = binarize_df(humours_processed)\n",
    "indirect_binary = binarize_df(indirect_processed)\n",
    "irony_binary = binarize_df(irony_processed)\n",
    "maxims_binary = binarize_df(maxims_processed)\n",
    "metaphor_binary = binarize_df(metaphor_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "75bbb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all into one cleaned file\n",
    "label_scores = pd.concat([coherence_binary,deceits_binary,humours_binary,indirect_binary,irony_binary, maxims_binary,metaphor_binary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "81ad930e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>shuffled_option_names</th>\n",
       "      <th>metric_formula</th>\n",
       "      <th>model_name</th>\n",
       "      <th>seed</th>\n",
       "      <th>item_id</th>\n",
       "      <th>phenomenon</th>\n",
       "      <th>metric</th>\n",
       "      <th>label_type</th>\n",
       "      <th>target</th>\n",
       "      <th>distractor_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean_sentence_cond_probs</td>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>coherence</td>\n",
       "      <td>label_score</td>\n",
       "      <td>label_alpha</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean_sentence_surprisal</td>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>coherence</td>\n",
       "      <td>label_score</td>\n",
       "      <td>label_alpha</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence_cond_probs</td>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>coherence</td>\n",
       "      <td>label_score</td>\n",
       "      <td>label_alpha</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence_mi</td>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>coherence</td>\n",
       "      <td>label_score</td>\n",
       "      <td>label_alpha</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence_mi_surprisal</td>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>coherence</td>\n",
       "      <td>label_score</td>\n",
       "      <td>label_alpha</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "shuffled_option_names            metric_formula         model_name  seed  \\\n",
       "0                      mean_sentence_cond_probs  google/flan-t5-xl     0   \n",
       "1                       mean_sentence_surprisal  google/flan-t5-xl     0   \n",
       "2                           sentence_cond_probs  google/flan-t5-xl     0   \n",
       "3                                   sentence_mi  google/flan-t5-xl     0   \n",
       "4                         sentence_mi_surprisal  google/flan-t5-xl     0   \n",
       "\n",
       "shuffled_option_names  item_id phenomenon       metric   label_type  target  \\\n",
       "0                            1  coherence  label_score  label_alpha     0.5   \n",
       "1                            1  coherence  label_score  label_alpha     0.5   \n",
       "2                            1  coherence  label_score  label_alpha     0.5   \n",
       "3                            1  coherence  label_score  label_alpha     0.5   \n",
       "4                            1  coherence  label_score  label_alpha     0.5   \n",
       "\n",
       "shuffled_option_names  distractor_prob  \n",
       "0                                  0.5  \n",
       "1                                  0.5  \n",
       "2                                  0.5  \n",
       "3                                  0.5  \n",
       "4                                  0.5  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label_scores.to_csv(\"../results/log_probs/label_scores_all_cleaned_binarized.csv\", index=False)\n",
    "label_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2e05875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse the data\n",
    "# averaging probabilities across items and across seeds, by metric, by model, by phenomenon, by label type\n",
    "scores_binary_summary = label_scores.groupby([\"model_name\", \"metric_formula\", \"phenomenon\", \"label_type\"]).mean([\"target\", \"distractor_prob\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3a3d112a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>shuffled_option_names</th>\n",
       "      <th>model_name</th>\n",
       "      <th>metric_formula</th>\n",
       "      <th>phenomenon</th>\n",
       "      <th>label_type</th>\n",
       "      <th>seed</th>\n",
       "      <th>item_id</th>\n",
       "      <th>target</th>\n",
       "      <th>distractor_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>mean_sentence_cond_probs</td>\n",
       "      <td>coherence</td>\n",
       "      <td>label_alpha</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>mean_sentence_cond_probs</td>\n",
       "      <td>coherence</td>\n",
       "      <td>label_numeric</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>mean_sentence_cond_probs</td>\n",
       "      <td>deceits</td>\n",
       "      <td>label_alpha</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>mean_sentence_cond_probs</td>\n",
       "      <td>deceits</td>\n",
       "      <td>label_numeric</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>mean_sentence_cond_probs</td>\n",
       "      <td>humour</td>\n",
       "      <td>label_alpha</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>sentence_surprisal</td>\n",
       "      <td>irony</td>\n",
       "      <td>label_numeric</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>sentence_surprisal</td>\n",
       "      <td>maxims</td>\n",
       "      <td>label_alpha</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>sentence_surprisal</td>\n",
       "      <td>maxims</td>\n",
       "      <td>label_numeric</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>sentence_surprisal</td>\n",
       "      <td>metaphor</td>\n",
       "      <td>label_alpha</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>sentence_surprisal</td>\n",
       "      <td>metaphor</td>\n",
       "      <td>label_numeric</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "shuffled_option_names         model_name            metric_formula phenomenon  \\\n",
       "0                      google/flan-t5-xl  mean_sentence_cond_probs  coherence   \n",
       "1                      google/flan-t5-xl  mean_sentence_cond_probs  coherence   \n",
       "2                      google/flan-t5-xl  mean_sentence_cond_probs    deceits   \n",
       "3                      google/flan-t5-xl  mean_sentence_cond_probs    deceits   \n",
       "4                      google/flan-t5-xl  mean_sentence_cond_probs     humour   \n",
       "..                                   ...                       ...        ...   \n",
       "79                     google/flan-t5-xl        sentence_surprisal      irony   \n",
       "80                     google/flan-t5-xl        sentence_surprisal     maxims   \n",
       "81                     google/flan-t5-xl        sentence_surprisal     maxims   \n",
       "82                     google/flan-t5-xl        sentence_surprisal   metaphor   \n",
       "83                     google/flan-t5-xl        sentence_surprisal   metaphor   \n",
       "\n",
       "shuffled_option_names     label_type  seed  item_id  target  distractor_prob  \n",
       "0                        label_alpha   2.0     20.5    0.50             0.50  \n",
       "1                      label_numeric   2.0     20.5    0.50             0.50  \n",
       "2                        label_alpha   2.0     10.5    0.25             0.75  \n",
       "3                      label_numeric   2.0     10.5    0.25             0.75  \n",
       "4                        label_alpha   2.0     13.0    0.20             0.80  \n",
       "..                               ...   ...      ...     ...              ...  \n",
       "79                     label_numeric   2.0     13.0    0.25             0.75  \n",
       "80                       label_alpha   2.0     10.0    0.25             0.75  \n",
       "81                     label_numeric   2.0     10.0    0.25             0.75  \n",
       "82                       label_alpha   2.0     10.5    0.20             0.80  \n",
       "83                     label_numeric   2.0     10.5    0.20             0.80  \n",
       "\n",
       "[84 rows x 8 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same thing by phenomenon\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d1773a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read human results for the \"correlation plot\"\n",
    "df_human_coherence = pd.read_csv(\"../data/human_results/Human_CoherenceInference.csv\") \n",
    "df_human_coherence[\"phenomenon\"] = \"coherence\"\n",
    "df_human_deceits = pd.read_csv(\"../data/human_results/Human_Deceits.csv\") \n",
    "df_human_deceits[\"phenomenon\"] = \"deceits\"\n",
    "df_human_humour = pd.read_csv(\"../data/human_results/Human_Humour.csv\") \n",
    "df_human_humour[\"phenomenon\"] = \"humour\"\n",
    "df_human_indirect = pd.read_csv(\"../data/human_results/Human_IndirectSpeech.csv\") \n",
    "df_human_indirect[\"phenomenon\"] = \"indirect_speech\"\n",
    "df_human_irony = pd.read_csv(\"../data/human_results/Human_Irony.csv\") \n",
    "df_human_irony[\"phenomenon\"] = \"irony\"\n",
    "df_human_metaphor = pd.read_csv(\"../data/human_results/Human_Metaphor.csv\") \n",
    "df_human_metaphor[\"phenomenon\"] = \"metaphor\"\n",
    "df_human_maxims = pd.read_csv(\"../data/human_results/Human_Maxims.csv\") \n",
    "df_human_maxims[\"phenomenon\"] = \"maxims\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a924117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human_all = pd.concat([df_human_coherence, df_human_deceits, df_human_humour, df_human_indirect, df_human_irony, df_human_metaphor, df_human_maxims])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "96c2bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human_summary = df_human_all.groupby([\"phenomenon\"]).mean(\"Correct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "32d2f489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phenomenon</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pKey</th>\n",
       "      <th>itemNum</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coherence</td>\n",
       "      <td>7480.5</td>\n",
       "      <td>274.371658</td>\n",
       "      <td>20.5</td>\n",
       "      <td>0.856684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deceits</td>\n",
       "      <td>3740.5</td>\n",
       "      <td>274.371658</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.841979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>humour</td>\n",
       "      <td>4675.5</td>\n",
       "      <td>274.371658</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.846845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indirect_speech</td>\n",
       "      <td>3740.5</td>\n",
       "      <td>274.371658</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.885963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>irony</td>\n",
       "      <td>4675.5</td>\n",
       "      <td>274.371658</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.919465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>maxims</td>\n",
       "      <td>3740.5</td>\n",
       "      <td>274.371658</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.795856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>metaphor</td>\n",
       "      <td>3740.5</td>\n",
       "      <td>274.371658</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.896791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        phenomenon  Unnamed: 0        pKey  itemNum   Correct\n",
       "0        coherence      7480.5  274.371658     20.5  0.856684\n",
       "1          deceits      3740.5  274.371658     10.5  0.841979\n",
       "2           humour      4675.5  274.371658     13.0  0.846845\n",
       "3  indirect_speech      3740.5  274.371658     10.5  0.885963\n",
       "4            irony      4675.5  274.371658     13.0  0.919465\n",
       "5           maxims      3740.5  274.371658     10.5  0.795856\n",
       "6         metaphor      3740.5  274.371658     10.5  0.896791"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_human_summary = df_human_summary.reset_index()\n",
    "df_human_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "bd2c1082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n",
      "ph  coherence\n",
      "ph  coherence\n",
      "ph  deceits\n",
      "ph  deceits\n",
      "ph  humour\n",
      "ph  humour\n",
      "ph  indirect_speech\n",
      "ph  indirect_speech\n",
      "ph  irony\n",
      "ph  irony\n",
      "ph  maxims\n",
      "ph  maxims\n",
      "ph  metaphor\n",
      "ph  metaphor\n"
     ]
    }
   ],
   "source": [
    "# add human proportion to model summary\n",
    "scores_binary_summary_wHuman = scores_binary_summary.copy()\n",
    "\n",
    "for i, r in scores_binary_summary_wHuman.iterrows():\n",
    "    phenomenon = r['phenomenon']\n",
    "    val = float(df_human_summary[df_human_summary[\"phenomenon\"] == phenomenon]['Correct'])\n",
    "    scores_binary_summary_wHuman.loc[i, 'human'] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "443cd136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>shuffled_option_names</th>\n",
       "      <th>model_name</th>\n",
       "      <th>metric_formula</th>\n",
       "      <th>phenomenon</th>\n",
       "      <th>label_type</th>\n",
       "      <th>seed</th>\n",
       "      <th>item_id</th>\n",
       "      <th>target</th>\n",
       "      <th>distractor_prob</th>\n",
       "      <th>human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>mean_sentence_cond_probs</td>\n",
       "      <td>coherence</td>\n",
       "      <td>label_alpha</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.856684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>mean_sentence_cond_probs</td>\n",
       "      <td>coherence</td>\n",
       "      <td>label_numeric</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.856684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>mean_sentence_cond_probs</td>\n",
       "      <td>deceits</td>\n",
       "      <td>label_alpha</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.841979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>mean_sentence_cond_probs</td>\n",
       "      <td>deceits</td>\n",
       "      <td>label_numeric</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.841979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google/flan-t5-xl</td>\n",
       "      <td>mean_sentence_cond_probs</td>\n",
       "      <td>humour</td>\n",
       "      <td>label_alpha</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.846845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>meta-llama/Llama-2-7b-hf</td>\n",
       "      <td>sentence_surprisal</td>\n",
       "      <td>irony</td>\n",
       "      <td>label_numeric</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.250496</td>\n",
       "      <td>0.749504</td>\n",
       "      <td>0.919465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>meta-llama/Llama-2-7b-hf</td>\n",
       "      <td>sentence_surprisal</td>\n",
       "      <td>maxims</td>\n",
       "      <td>label_alpha</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.251931</td>\n",
       "      <td>0.748069</td>\n",
       "      <td>0.795856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>meta-llama/Llama-2-7b-hf</td>\n",
       "      <td>sentence_surprisal</td>\n",
       "      <td>maxims</td>\n",
       "      <td>label_numeric</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.251008</td>\n",
       "      <td>0.748992</td>\n",
       "      <td>0.795856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>meta-llama/Llama-2-7b-hf</td>\n",
       "      <td>sentence_surprisal</td>\n",
       "      <td>metaphor</td>\n",
       "      <td>label_alpha</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.199456</td>\n",
       "      <td>0.800544</td>\n",
       "      <td>0.896791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>meta-llama/Llama-2-7b-hf</td>\n",
       "      <td>sentence_surprisal</td>\n",
       "      <td>metaphor</td>\n",
       "      <td>label_numeric</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.200949</td>\n",
       "      <td>0.799051</td>\n",
       "      <td>0.896791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "shuffled_option_names                model_name            metric_formula  \\\n",
       "0                             google/flan-t5-xl  mean_sentence_cond_probs   \n",
       "1                             google/flan-t5-xl  mean_sentence_cond_probs   \n",
       "2                             google/flan-t5-xl  mean_sentence_cond_probs   \n",
       "3                             google/flan-t5-xl  mean_sentence_cond_probs   \n",
       "4                             google/flan-t5-xl  mean_sentence_cond_probs   \n",
       "..                                          ...                       ...   \n",
       "247                    meta-llama/Llama-2-7b-hf        sentence_surprisal   \n",
       "248                    meta-llama/Llama-2-7b-hf        sentence_surprisal   \n",
       "249                    meta-llama/Llama-2-7b-hf        sentence_surprisal   \n",
       "250                    meta-llama/Llama-2-7b-hf        sentence_surprisal   \n",
       "251                    meta-llama/Llama-2-7b-hf        sentence_surprisal   \n",
       "\n",
       "shuffled_option_names phenomenon     label_type  seed  item_id    target  \\\n",
       "0                      coherence    label_alpha   2.0     20.5  0.500000   \n",
       "1                      coherence  label_numeric   2.0     20.5  0.500000   \n",
       "2                        deceits    label_alpha   2.0     10.5  0.250000   \n",
       "3                        deceits  label_numeric   2.0     10.5  0.250000   \n",
       "4                         humour    label_alpha   2.0     13.0  0.200000   \n",
       "..                           ...            ...   ...      ...       ...   \n",
       "247                        irony  label_numeric   2.0     13.0  0.250496   \n",
       "248                       maxims    label_alpha   2.0     10.0  0.251931   \n",
       "249                       maxims  label_numeric   2.0     10.0  0.251008   \n",
       "250                     metaphor    label_alpha   2.0     10.5  0.199456   \n",
       "251                     metaphor  label_numeric   2.0     10.5  0.200949   \n",
       "\n",
       "shuffled_option_names  distractor_prob     human  \n",
       "0                             0.500000  0.856684  \n",
       "1                             0.500000  0.856684  \n",
       "2                             0.750000  0.841979  \n",
       "3                             0.750000  0.841979  \n",
       "4                             0.800000  0.846845  \n",
       "..                                 ...       ...  \n",
       "247                           0.749504  0.919465  \n",
       "248                           0.748069  0.795856  \n",
       "249                           0.748992  0.795856  \n",
       "250                           0.800544  0.896791  \n",
       "251                           0.799051  0.896791  \n",
       "\n",
       "[252 rows x 9 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plotting results by metric, by phenomenon, by model, by label type \n",
    "# TODO\n",
    "scores_binary_summary_wHuman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c411a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speakers39",
   "language": "python",
   "name": "speakers39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
